{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:10:38.503477Z",
     "start_time": "2024-04-29T02:10:38.489361100Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from deepmorpho.dl_folder.data_classes.morpho_dataset import EmbryoDataset\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def connect_to_database(db_path):\n",
    "    \"\"\"Подключение к базе данных SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(\"Успешно подключились к базе данных\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Ошибка при подключении к SQLite: {e}\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:30.454841300Z",
     "start_time": "2024-04-29T02:00:30.419772500Z"
    }
   },
   "id": "7859ea86af9ddc3d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def extract_data(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        query = \"SELECT wtf_wtl_id, wtf_frame, wtf_rcnn_text FROM well_timeline_frames;\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        images = []\n",
    "        annotations = []\n",
    "        for wtf_wtl_id, blob_data, json_data in data:\n",
    "            image_bytes = io.BytesIO(blob_data)\n",
    "            image = Image.open(image_bytes)\n",
    "            try:\n",
    "                annotation = json.loads(json_data)\n",
    "                images.append((wtf_wtl_id, image))\n",
    "                annotations.append((wtf_wtl_id, annotation))\n",
    "            except TypeError:\n",
    "                continue\n",
    "        return images, annotations\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Ошибка при выполнении SQL: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Ошибка при открытии изображения: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:32.811982100Z",
     "start_time": "2024-04-29T02:00:32.796553700Z"
    }
   },
   "id": "9830ce2cfa7b8223"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успешно подключились к базе данных\n"
     ]
    }
   ],
   "source": [
    "db_path = 'C:\\Work\\CLASSES\\SPRING2024\\DeepMorphoDynamics\\deepmorpho\\so_deep.db'  \n",
    "conn = connect_to_database(db_path)\n",
    "if conn:\n",
    "    images, annotations = extract_data(conn)\n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Не удалось подключиться к базе данных.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:48.209003100Z",
     "start_time": "2024-04-29T02:00:35.312139700Z"
    }
   },
   "id": "32dd7be3c8219716"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 461, 'label': 'BLFL', 'prediction': 0.6353023648262024, 'bbox': [210, 118, 425, 323]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "(461, <PIL.BmpImagePlugin.BmpImageFile image mode=L size=500x500>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for annotation in annotations:\n",
    "    id = annotation[0]  \n",
    "    data = annotation[1]  \n",
    "    highest_prediction = max(data['predictions'], key=lambda x: x['prediction'])\n",
    "    label = highest_prediction['label']\n",
    "    prediction = highest_prediction['prediction']\n",
    "    bbox = data['bboxes'][0]  \n",
    "    result.append({'ID': id, 'label': label, 'prediction': prediction, 'bbox': bbox})\n",
    "\n",
    "print(result[0])\n",
    "images[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:53.948723100Z",
     "start_time": "2024-04-29T02:00:53.867093500Z"
    }
   },
   "id": "e51c2812b07da580"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes = set()\n",
    "\n",
    "for annotation in annotations:\n",
    "    data = annotation[1]  \n",
    "    for prediction in data['predictions']:\n",
    "        all_classes.add(prediction['label'])\n",
    "\n",
    "len(all_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:55.926490900Z",
     "start_time": "2024-04-29T02:00:55.872653400Z"
    }
   },
   "id": "cf76345c22fd2ef5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we prepare dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aaaae13dee3e7aa"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:00:57.977031300Z",
     "start_time": "2024-04-29T02:00:57.959688Z"
    }
   },
   "id": "338053e2c684879e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset = EmbryoDataset(result, images, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_indices, test_indices, _, _ = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    dataset.labels, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:02:27.158687700Z",
     "start_time": "2024-04-29T02:02:27.129120Z"
    }
   },
   "id": "bbbf4ae15146c3b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, VIS transformer. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed5cfb970be85b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-29T01:53:29.983634200Z"
    }
   },
   "id": "3c9cf804a0a466c4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_subset = Subset(dataset, train_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(all_classes))\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:15:03.588196600Z",
     "start_time": "2024-04-29T02:15:01.772979100Z"
    }
   },
   "id": "d7a6cd07da5c7cf7"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataloader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    model.train()  \n",
    "    return 100 * correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:15:34.658678600Z",
     "start_time": "2024-04-29T02:15:34.649624300Z"
    }
   },
   "id": "40a6cf6d0625d428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch n: 1/3\n",
      "----------\n",
      "Working on batch  1/290\n",
      "Working on batch  2/290\n",
      "Working on batch  3/290\n",
      "Working on batch  4/290\n",
      "Working on batch  5/290\n",
      "Working on batch  6/290\n",
      "Working on batch  7/290\n",
      "Working on batch  8/290\n",
      "Working on batch  9/290\n",
      "Working on batch  10/290\n",
      "Working on batch  11/290\n",
      "Working on batch  12/290\n",
      "Working on batch  13/290\n",
      "Working on batch  14/290\n",
      "Working on batch  15/290\n",
      "Working on batch  16/290\n",
      "Working on batch  17/290\n",
      "Working on batch  18/290\n",
      "Working on batch  19/290\n",
      "Working on batch  20/290\n",
      "Working on batch  21/290\n",
      "Working on batch  22/290\n",
      "Working on batch  23/290\n",
      "Working on batch  24/290\n",
      "Working on batch  25/290\n",
      "Working on batch  26/290\n",
      "Working on batch  27/290\n",
      "Working on batch  28/290\n",
      "Working on batch  29/290\n",
      "Working on batch  30/290\n",
      "Working on batch  31/290\n",
      "Working on batch  32/290\n",
      "Working on batch  33/290\n",
      "Working on batch  34/290\n",
      "Working on batch  35/290\n",
      "Working on batch  36/290\n",
      "Working on batch  37/290\n",
      "Working on batch  38/290\n",
      "Working on batch  39/290\n",
      "Working on batch  40/290\n",
      "Working on batch  41/290\n",
      "Working on batch  42/290\n",
      "Working on batch  43/290\n",
      "Working on batch  44/290\n",
      "Working on batch  45/290\n",
      "Working on batch  46/290\n",
      "Working on batch  47/290\n",
      "Working on batch  48/290\n",
      "Working on batch  49/290\n",
      "Working on batch  50/290\n",
      "Working on batch  51/290\n",
      "Working on batch  52/290\n",
      "Working on batch  53/290\n",
      "Working on batch  54/290\n",
      "Working on batch  55/290\n",
      "Working on batch  56/290\n",
      "Working on batch  57/290\n",
      "Working on batch  58/290\n",
      "Working on batch  59/290\n",
      "Working on batch  60/290\n",
      "Working on batch  61/290\n",
      "Working on batch  62/290\n",
      "Working on batch  63/290\n",
      "Working on batch  64/290\n",
      "Working on batch  65/290\n",
      "Working on batch  66/290\n",
      "Working on batch  67/290\n",
      "Working on batch  68/290\n",
      "Working on batch  69/290\n",
      "Working on batch  70/290\n",
      "Working on batch  71/290\n",
      "Working on batch  72/290\n",
      "Working on batch  73/290\n",
      "Working on batch  74/290\n",
      "Working on batch  75/290\n",
      "Working on batch  76/290\n",
      "Working on batch  77/290\n",
      "Working on batch  78/290\n",
      "Working on batch  79/290\n",
      "Working on batch  80/290\n",
      "Working on batch  81/290\n",
      "Working on batch  82/290\n",
      "Working on batch  83/290\n",
      "Working on batch  84/290\n",
      "Working on batch  85/290\n",
      "Working on batch  86/290\n",
      "Working on batch  87/290\n",
      "Working on batch  88/290\n",
      "Working on batch  89/290\n",
      "Working on batch  90/290\n",
      "Working on batch  91/290\n",
      "Working on batch  92/290\n",
      "Working on batch  93/290\n",
      "Working on batch  94/290\n",
      "Working on batch  95/290\n",
      "Working on batch  96/290\n",
      "Working on batch  97/290\n",
      "Working on batch  98/290\n",
      "Working on batch  99/290\n",
      "Working on batch  100/290\n",
      "Working on batch  101/290\n",
      "Working on batch  102/290\n",
      "Working on batch  103/290\n",
      "Working on batch  104/290\n",
      "Working on batch  105/290\n",
      "Working on batch  106/290\n",
      "Working on batch  107/290\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch n: {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        print(f'Working on batch  {batch_idx+1}/{len(train_dataloader)}')\n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model.state_dict(), f'{model_dir}/model_epoch_{epoch+1}.pth')\n",
    "    accuracy = evaluate_accuracy(test_dataloader, model)\n",
    "    print(f'Epoch [{epoch+1}/3]'\n",
    "          f'Loss: {loss.item()}, Accuracy: {accuracy}%') \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-29T02:28:49.699255100Z"
    }
   },
   "id": "a74a24d364ca7863"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def plot_roc_curve(dataloader):\n",
    "    model.eval()\n",
    "    test_probs = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            test_probs.extend(probabilities[:, 1].cpu().numpy())\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_targets, test_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T02:09:18.508710300Z",
     "start_time": "2024-04-29T02:09:18.502098300Z"
    }
   },
   "id": "4ae00bfd543f3e3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39de72f095e3b0e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
