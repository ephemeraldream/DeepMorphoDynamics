{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-29T10:10:23.088226800Z",
     "start_time": "2024-04-29T10:10:10.337773Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from deepmorpho.dl_folder.data_classes.morpho_dataset import EmbryoDataset\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def connect_to_database(db_path):\n",
    "    \"\"\"Подключение к базе данных SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(\"Успешно подключились к базе данных\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Ошибка при подключении к SQLite: {e}\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T10:09:55.628783500Z",
     "start_time": "2024-04-29T10:09:55.617028100Z"
    }
   },
   "id": "7859ea86af9ddc3d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_data(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        query = \"SELECT wtf_wtl_id, wtf_frame, wtf_rcnn_text FROM well_timeline_frames;\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        images = []\n",
    "        annotations = []\n",
    "        for wtf_wtl_id, blob_data, json_data in data:\n",
    "            image_bytes = io.BytesIO(blob_data)\n",
    "            image = Image.open(image_bytes)\n",
    "            try:\n",
    "                annotation = json.loads(json_data)\n",
    "                images.append((wtf_wtl_id, image))\n",
    "                annotations.append((wtf_wtl_id, annotation))\n",
    "            except TypeError:\n",
    "                continue\n",
    "        return images, annotations\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Ошибка при выполнении SQL: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Ошибка при открытии изображения: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:26:55.753884Z",
     "start_time": "2024-04-29T06:26:42.492376100Z"
    }
   },
   "id": "9830ce2cfa7b8223"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlite3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m, in \u001B[0;36mconnect_to_database\u001B[1;34m(db_path)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     conn \u001B[38;5;241m=\u001B[39m \u001B[43msqlite3\u001B[49m\u001B[38;5;241m.\u001B[39mconnect(db_path)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mУспешно подключились к базе данных\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sqlite3' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m db_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mWork\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mCLASSES\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mSPRING2024\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDeepMorphoDynamics\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdeepmorpho\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mso_deep.db\u001B[39m\u001B[38;5;124m'\u001B[39m  \n\u001B[1;32m----> 2\u001B[0m conn \u001B[38;5;241m=\u001B[39m \u001B[43mconnect_to_database\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdb_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn:\n\u001B[0;32m      4\u001B[0m     images, annotations \u001B[38;5;241m=\u001B[39m extract_data(conn)\n",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m, in \u001B[0;36mconnect_to_database\u001B[1;34m(db_path)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mУспешно подключились к базе данных\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m conn\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[43msqlite3\u001B[49m\u001B[38;5;241m.\u001B[39mError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mОшибка при подключении к SQLite: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sqlite3' is not defined"
     ]
    }
   ],
   "source": [
    "db_path = 'C:\\Work\\CLASSES\\SPRING2024\\DeepMorphoDynamics\\deepmorpho\\so_deep.db'  \n",
    "conn = connect_to_database(db_path)\n",
    "if conn:\n",
    "    images, annotations = extract_data(conn)\n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"Не удалось подключиться к базе данных.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T10:10:06.797379600Z",
     "start_time": "2024-04-29T10:10:06.035581400Z"
    }
   },
   "id": "32dd7be3c8219716"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': 461, 'label': 'BLFL', 'prediction': 0.6353023648262024, 'bbox': [210, 118, 425, 323]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "(461, <PIL.BmpImagePlugin.BmpImageFile image mode=L size=500x500>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for annotation in annotations:\n",
    "    id = annotation[0]  \n",
    "    data = annotation[1]  \n",
    "    highest_prediction = max(data['predictions'], key=lambda x: x['prediction'])\n",
    "    label = highest_prediction['label']\n",
    "    prediction = highest_prediction['prediction']\n",
    "    bbox = data['bboxes'][0]  \n",
    "    result.append({'ID': id, 'label': label, 'prediction': prediction, 'bbox': bbox})\n",
    "\n",
    "print(result[0])\n",
    "images[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:06.797254500Z",
     "start_time": "2024-04-29T06:27:06.723362600Z"
    }
   },
   "id": "e51c2812b07da580"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes = set()\n",
    "\n",
    "for annotation in annotations:\n",
    "    data = annotation[1]  \n",
    "    for prediction in data['predictions']:\n",
    "        all_classes.add(prediction['label'])\n",
    "\n",
    "len(all_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:06.834369200Z",
     "start_time": "2024-04-29T06:27:06.780736900Z"
    }
   },
   "id": "cf76345c22fd2ef5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we prepare dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aaaae13dee3e7aa"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:06.834874100Z",
     "start_time": "2024-04-29T06:27:06.818111300Z"
    }
   },
   "id": "338053e2c684879e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset = EmbryoDataset(result, images, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_indices, test_indices, _, _ = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    dataset.labels, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:06.835881400Z",
     "start_time": "2024-04-29T06:27:06.822625200Z"
    }
   },
   "id": "bbbf4ae15146c3b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, VIS transformer. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed5cfb970be85b7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:06.835881400Z",
     "start_time": "2024-04-29T06:27:06.834874100Z"
    }
   },
   "id": "3c9cf804a0a466c4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_subset = Subset(dataset, train_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(all_classes))\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:08.343050300Z",
     "start_time": "2024-04-29T06:27:06.840918Z"
    }
   },
   "id": "d7a6cd07da5c7cf7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataloader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    model.train()  \n",
    "    return 100 * correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T06:27:08.346874600Z",
     "start_time": "2024-04-29T06:27:08.344359Z"
    }
   },
   "id": "40a6cf6d0625d428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch n: 1/3\n",
      "----------\n",
      "Working on batch  1/290\n",
      "Loss: 2.834705114364624\n",
      "Working on batch  2/290\n",
      "Loss: 1.9630963802337646\n",
      "Working on batch  3/290\n",
      "Loss: 4.22343635559082\n",
      "Working on batch  4/290\n",
      "Loss: 2.6772537231445312\n",
      "Working on batch  5/290\n",
      "Loss: 1.269192099571228\n",
      "Working on batch  6/290\n",
      "Loss: 1.5225821733474731\n",
      "Working on batch  7/290\n",
      "Loss: 1.7879706621170044\n",
      "Working on batch  8/290\n",
      "Loss: 1.4279738664627075\n",
      "Working on batch  9/290\n",
      "Loss: 2.5996053218841553\n",
      "Working on batch  10/290\n",
      "Loss: 2.011125326156616\n",
      "Working on batch  11/290\n",
      "Loss: 2.6865453720092773\n",
      "Working on batch  12/290\n",
      "Loss: 1.8920385837554932\n",
      "Working on batch  13/290\n",
      "Loss: 1.9380030632019043\n",
      "Working on batch  14/290\n",
      "Loss: 2.3911614418029785\n",
      "Working on batch  15/290\n",
      "Loss: 1.803059458732605\n",
      "Working on batch  16/290\n",
      "Loss: 1.412466049194336\n",
      "Working on batch  17/290\n",
      "Loss: 1.6524972915649414\n",
      "Working on batch  18/290\n",
      "Loss: 1.6621967554092407\n",
      "Working on batch  19/290\n",
      "Loss: 1.81128990650177\n",
      "Working on batch  20/290\n",
      "Loss: 1.6213709115982056\n",
      "Working on batch  21/290\n",
      "Loss: 1.6420027017593384\n",
      "Working on batch  22/290\n",
      "Loss: 1.355398416519165\n",
      "Working on batch  23/290\n",
      "Loss: 1.40812087059021\n",
      "Working on batch  24/290\n",
      "Loss: 1.5341542959213257\n",
      "Working on batch  25/290\n",
      "Loss: 1.391327142715454\n",
      "Working on batch  26/290\n",
      "Loss: 1.720041036605835\n",
      "Working on batch  27/290\n",
      "Loss: 1.585423469543457\n",
      "Working on batch  28/290\n",
      "Loss: 1.4403778314590454\n",
      "Working on batch  29/290\n",
      "Loss: 1.5228196382522583\n",
      "Working on batch  30/290\n",
      "Loss: 1.3312227725982666\n",
      "Working on batch  31/290\n",
      "Loss: 1.7413651943206787\n",
      "Working on batch  32/290\n",
      "Loss: 2.209454298019409\n",
      "Working on batch  33/290\n",
      "Loss: 1.4378622770309448\n",
      "Working on batch  34/290\n",
      "Loss: 1.3816877603530884\n",
      "Working on batch  35/290\n",
      "Loss: 1.599107027053833\n",
      "Working on batch  36/290\n",
      "Loss: 1.4330861568450928\n",
      "Working on batch  37/290\n",
      "Loss: 1.455013632774353\n",
      "Working on batch  38/290\n",
      "Loss: 1.8326456546783447\n",
      "Working on batch  39/290\n",
      "Loss: 1.672904133796692\n",
      "Working on batch  40/290\n",
      "Loss: 1.4504387378692627\n",
      "Working on batch  41/290\n",
      "Loss: 1.7875561714172363\n",
      "Working on batch  42/290\n",
      "Loss: 1.3861587047576904\n",
      "Working on batch  43/290\n",
      "Loss: 1.3839925527572632\n",
      "Working on batch  44/290\n",
      "Loss: 1.3830902576446533\n",
      "Working on batch  45/290\n",
      "Loss: 1.5702588558197021\n",
      "Working on batch  46/290\n",
      "Loss: 1.3828215599060059\n",
      "Working on batch  47/290\n",
      "Loss: 1.4955942630767822\n",
      "Working on batch  48/290\n",
      "Loss: 1.8829761743545532\n",
      "Working on batch  49/290\n",
      "Loss: 1.3146167993545532\n",
      "Working on batch  50/290\n",
      "Loss: 1.2285418510437012\n",
      "Working on batch  51/290\n",
      "Loss: 1.298807978630066\n",
      "Working on batch  52/290\n",
      "Loss: 1.5683865547180176\n",
      "Working on batch  53/290\n",
      "Loss: 1.5055674314498901\n",
      "Working on batch  54/290\n",
      "Loss: 1.4116740226745605\n",
      "Working on batch  55/290\n",
      "Loss: 1.3114036321640015\n",
      "Working on batch  56/290\n",
      "Loss: 1.5197073221206665\n",
      "Working on batch  57/290\n",
      "Loss: 1.3039867877960205\n",
      "Working on batch  58/290\n",
      "Loss: 1.389113426208496\n",
      "Working on batch  59/290\n",
      "Loss: 1.4444401264190674\n",
      "Working on batch  60/290\n",
      "Loss: 1.446349859237671\n",
      "Working on batch  61/290\n",
      "Loss: 1.6298987865447998\n",
      "Working on batch  62/290\n",
      "Loss: 1.2919822931289673\n",
      "Working on batch  63/290\n",
      "Loss: 1.837697982788086\n",
      "Working on batch  64/290\n",
      "Loss: 1.5727496147155762\n",
      "Working on batch  65/290\n",
      "Loss: 1.4332281351089478\n",
      "Working on batch  66/290\n",
      "Loss: 1.4083476066589355\n",
      "Working on batch  67/290\n",
      "Loss: 1.2885451316833496\n",
      "Working on batch  68/290\n",
      "Loss: 1.80864679813385\n",
      "Working on batch  69/290\n",
      "Loss: 1.5765223503112793\n",
      "Working on batch  70/290\n",
      "Loss: 1.6062523126602173\n",
      "Working on batch  71/290\n",
      "Loss: 1.7568868398666382\n",
      "Working on batch  72/290\n",
      "Loss: 1.405728816986084\n",
      "Working on batch  73/290\n",
      "Loss: 1.4904394149780273\n",
      "Working on batch  74/290\n",
      "Loss: 1.3807806968688965\n",
      "Working on batch  75/290\n",
      "Loss: 1.316853642463684\n",
      "Working on batch  76/290\n",
      "Loss: 1.4335650205612183\n",
      "Working on batch  77/290\n",
      "Loss: 1.414888858795166\n",
      "Working on batch  78/290\n",
      "Loss: 1.4104348421096802\n",
      "Working on batch  79/290\n",
      "Loss: 1.2793315649032593\n",
      "Working on batch  80/290\n",
      "Loss: 1.4227155447006226\n",
      "Working on batch  81/290\n",
      "Loss: 1.3900632858276367\n",
      "Working on batch  82/290\n",
      "Loss: 1.1567875146865845\n",
      "Working on batch  83/290\n",
      "Loss: 1.4332305192947388\n",
      "Working on batch  84/290\n",
      "Loss: 1.6112067699432373\n",
      "Working on batch  85/290\n",
      "Loss: 1.4964812994003296\n",
      "Working on batch  86/290\n",
      "Loss: 1.4785573482513428\n",
      "Working on batch  87/290\n",
      "Loss: 1.3442013263702393\n",
      "Working on batch  88/290\n",
      "Loss: 1.314797282218933\n",
      "Working on batch  89/290\n",
      "Loss: 1.1333658695220947\n",
      "Working on batch  90/290\n",
      "Loss: 1.1790375709533691\n",
      "Working on batch  91/290\n",
      "Loss: 1.291300892829895\n",
      "Working on batch  92/290\n",
      "Loss: 1.4771097898483276\n",
      "Working on batch  93/290\n",
      "Loss: 1.2739202976226807\n",
      "Working on batch  94/290\n",
      "Loss: 1.01548171043396\n",
      "Working on batch  95/290\n",
      "Loss: 1.4304534196853638\n",
      "Working on batch  96/290\n",
      "Loss: 1.569480061531067\n",
      "Working on batch  97/290\n",
      "Loss: 1.1577755212783813\n",
      "Working on batch  98/290\n",
      "Loss: 1.1268212795257568\n",
      "Working on batch  99/290\n",
      "Loss: 1.2060542106628418\n",
      "Working on batch  100/290\n",
      "Loss: 1.4244160652160645\n",
      "Working on batch  101/290\n",
      "Loss: 1.370056390762329\n",
      "Working on batch  102/290\n",
      "Loss: 2.0125553607940674\n",
      "Working on batch  103/290\n",
      "Loss: 1.2700165510177612\n",
      "Working on batch  104/290\n",
      "Loss: 1.073313593864441\n",
      "Working on batch  105/290\n",
      "Loss: 1.5319288969039917\n",
      "Working on batch  106/290\n",
      "Loss: 1.3900001049041748\n",
      "Working on batch  107/290\n",
      "Loss: 1.1968520879745483\n",
      "Working on batch  108/290\n",
      "Loss: 1.4762402772903442\n",
      "Working on batch  109/290\n",
      "Loss: 1.1730879545211792\n",
      "Working on batch  110/290\n",
      "Loss: 1.5242453813552856\n",
      "Working on batch  111/290\n",
      "Loss: 1.2435035705566406\n",
      "Working on batch  112/290\n",
      "Loss: 1.0003501176834106\n",
      "Working on batch  113/290\n",
      "Loss: 1.2650285959243774\n",
      "Working on batch  114/290\n",
      "Loss: 1.2672162055969238\n",
      "Working on batch  115/290\n",
      "Loss: 1.2524605989456177\n",
      "Working on batch  116/290\n",
      "Loss: 1.2760367393493652\n",
      "Working on batch  117/290\n",
      "Loss: 1.2886996269226074\n",
      "Working on batch  118/290\n",
      "Loss: 1.3310692310333252\n",
      "Working on batch  119/290\n",
      "Loss: 1.2865898609161377\n",
      "Working on batch  120/290\n",
      "Loss: 1.019086480140686\n",
      "Working on batch  121/290\n",
      "Loss: 1.309476375579834\n",
      "Working on batch  122/290\n",
      "Loss: 1.3682678937911987\n",
      "Working on batch  123/290\n",
      "Loss: 1.187267780303955\n",
      "Working on batch  124/290\n",
      "Loss: 1.257345199584961\n",
      "Working on batch  125/290\n",
      "Loss: 1.1160616874694824\n",
      "Working on batch  126/290\n",
      "Loss: 1.0664232969284058\n",
      "Working on batch  127/290\n",
      "Loss: 1.1519286632537842\n",
      "Working on batch  128/290\n",
      "Loss: 1.0216716527938843\n",
      "Working on batch  129/290\n",
      "Loss: 1.1591233015060425\n",
      "Working on batch  130/290\n",
      "Loss: 1.0907996892929077\n",
      "Working on batch  131/290\n",
      "Loss: 1.2825278043746948\n",
      "Working on batch  132/290\n",
      "Loss: 1.0184589624404907\n",
      "Working on batch  133/290\n",
      "Loss: 1.0899415016174316\n",
      "Working on batch  134/290\n",
      "Loss: 1.2361711263656616\n",
      "Working on batch  135/290\n",
      "Loss: 0.9238616824150085\n",
      "Working on batch  136/290\n",
      "Loss: 0.7632996439933777\n",
      "Working on batch  137/290\n",
      "Loss: 0.9215702414512634\n",
      "Working on batch  138/290\n",
      "Loss: 1.2996829748153687\n",
      "Working on batch  139/290\n",
      "Loss: 1.215704083442688\n",
      "Working on batch  140/290\n",
      "Loss: 1.454123616218567\n",
      "Working on batch  141/290\n",
      "Loss: 1.508387565612793\n",
      "Working on batch  142/290\n",
      "Loss: 1.3488084077835083\n",
      "Working on batch  143/290\n",
      "Loss: 1.050226092338562\n",
      "Working on batch  144/290\n",
      "Loss: 0.9759585857391357\n",
      "Working on batch  145/290\n",
      "Loss: 1.2984482049942017\n",
      "Working on batch  146/290\n",
      "Loss: 1.6350146532058716\n",
      "Working on batch  147/290\n",
      "Loss: 1.5681791305541992\n",
      "Working on batch  148/290\n",
      "Loss: 1.5124738216400146\n",
      "Working on batch  149/290\n",
      "Loss: 1.1975955963134766\n",
      "Working on batch  150/290\n",
      "Loss: 1.4436352252960205\n",
      "Working on batch  151/290\n",
      "Loss: 1.6399023532867432\n",
      "Working on batch  152/290\n",
      "Loss: 1.359529733657837\n",
      "Working on batch  153/290\n",
      "Loss: 1.2445788383483887\n",
      "Working on batch  154/290\n",
      "Loss: 1.2831130027770996\n",
      "Working on batch  155/290\n",
      "Loss: 1.0583312511444092\n",
      "Working on batch  156/290\n",
      "Loss: 1.4666153192520142\n",
      "Working on batch  157/290\n",
      "Loss: 1.4631657600402832\n",
      "Working on batch  158/290\n",
      "Loss: 0.974517822265625\n",
      "Working on batch  159/290\n",
      "Loss: 0.9868146777153015\n",
      "Working on batch  160/290\n",
      "Loss: 1.2040692567825317\n",
      "Working on batch  161/290\n",
      "Loss: 1.314250111579895\n",
      "Working on batch  162/290\n",
      "Loss: 0.9949507713317871\n",
      "Working on batch  163/290\n",
      "Loss: 1.4461973905563354\n",
      "Working on batch  164/290\n",
      "Loss: 1.1010855436325073\n",
      "Working on batch  165/290\n",
      "Loss: 1.192252278327942\n",
      "Working on batch  166/290\n",
      "Loss: 0.9869419932365417\n",
      "Working on batch  167/290\n",
      "Loss: 1.2703707218170166\n",
      "Working on batch  168/290\n",
      "Loss: 0.9164317846298218\n",
      "Working on batch  169/290\n",
      "Loss: 0.9735282063484192\n",
      "Working on batch  170/290\n",
      "Loss: 1.2385205030441284\n",
      "Working on batch  171/290\n",
      "Loss: 1.1728519201278687\n",
      "Working on batch  172/290\n",
      "Loss: 1.0275161266326904\n",
      "Working on batch  173/290\n",
      "Loss: 1.3355743885040283\n",
      "Working on batch  174/290\n",
      "Loss: 1.5435690879821777\n",
      "Working on batch  175/290\n",
      "Loss: 1.2113771438598633\n",
      "Working on batch  176/290\n",
      "Loss: 1.021399974822998\n",
      "Working on batch  177/290\n",
      "Loss: 1.1288963556289673\n",
      "Working on batch  178/290\n",
      "Loss: 1.1949608325958252\n",
      "Working on batch  179/290\n",
      "Loss: 1.2783405780792236\n",
      "Working on batch  180/290\n",
      "Loss: 1.041979432106018\n",
      "Working on batch  181/290\n",
      "Loss: 1.1211557388305664\n",
      "Working on batch  182/290\n",
      "Loss: 1.0042078495025635\n",
      "Working on batch  183/290\n",
      "Loss: 1.097843050956726\n",
      "Working on batch  184/290\n",
      "Loss: 1.4927009344100952\n",
      "Working on batch  185/290\n",
      "Loss: 0.8464323282241821\n",
      "Working on batch  186/290\n",
      "Loss: 1.7523635625839233\n",
      "Working on batch  187/290\n",
      "Loss: 0.9413959383964539\n",
      "Working on batch  188/290\n",
      "Loss: 1.261391520500183\n",
      "Working on batch  189/290\n",
      "Loss: 1.1124199628829956\n",
      "Working on batch  190/290\n",
      "Loss: 1.4870997667312622\n",
      "Working on batch  191/290\n",
      "Loss: 0.9756048321723938\n",
      "Working on batch  192/290\n",
      "Loss: 1.268858551979065\n",
      "Working on batch  193/290\n",
      "Loss: 0.7936898469924927\n",
      "Working on batch  194/290\n",
      "Loss: 1.071856141090393\n",
      "Working on batch  195/290\n",
      "Loss: 1.1711117029190063\n",
      "Working on batch  196/290\n",
      "Loss: 1.1365227699279785\n",
      "Working on batch  197/290\n",
      "Loss: 0.7284089922904968\n",
      "Working on batch  198/290\n",
      "Loss: 1.094024419784546\n",
      "Working on batch  199/290\n",
      "Loss: 0.8779133558273315\n",
      "Working on batch  200/290\n",
      "Loss: 1.126269817352295\n",
      "Working on batch  201/290\n",
      "Loss: 1.5466278791427612\n",
      "Working on batch  202/290\n",
      "Loss: 0.9317803978919983\n",
      "Working on batch  203/290\n",
      "Loss: 1.0397628545761108\n",
      "Working on batch  204/290\n",
      "Loss: 1.3531396389007568\n",
      "Working on batch  205/290\n",
      "Loss: 1.0798767805099487\n",
      "Working on batch  206/290\n",
      "Loss: 1.1652352809906006\n",
      "Working on batch  207/290\n",
      "Loss: 1.0264967679977417\n",
      "Working on batch  208/290\n",
      "Loss: 1.4513474702835083\n",
      "Working on batch  209/290\n",
      "Loss: 1.1255933046340942\n",
      "Working on batch  210/290\n",
      "Loss: 1.1874313354492188\n",
      "Working on batch  211/290\n",
      "Loss: 1.2938505411148071\n",
      "Working on batch  212/290\n",
      "Loss: 1.100806713104248\n",
      "Working on batch  213/290\n",
      "Loss: 1.1726348400115967\n",
      "Working on batch  214/290\n",
      "Loss: 1.1974354982376099\n",
      "Working on batch  215/290\n",
      "Loss: 0.8852546811103821\n",
      "Working on batch  216/290\n",
      "Loss: 1.212841510772705\n",
      "Working on batch  217/290\n",
      "Loss: 1.3474037647247314\n",
      "Working on batch  218/290\n",
      "Loss: 0.8191933035850525\n",
      "Working on batch  219/290\n",
      "Loss: 1.1251280307769775\n",
      "Working on batch  220/290\n",
      "Loss: 0.9161113500595093\n",
      "Working on batch  221/290\n",
      "Loss: 1.0790584087371826\n",
      "Working on batch  222/290\n",
      "Loss: 0.7304880619049072\n",
      "Working on batch  223/290\n",
      "Loss: 1.3131731748580933\n",
      "Working on batch  224/290\n",
      "Loss: 1.203294038772583\n",
      "Working on batch  225/290\n",
      "Loss: 0.9216834306716919\n",
      "Working on batch  226/290\n",
      "Loss: 0.9691382050514221\n",
      "Working on batch  227/290\n",
      "Loss: 1.2438220977783203\n",
      "Working on batch  228/290\n",
      "Loss: 1.232466697692871\n",
      "Working on batch  229/290\n",
      "Loss: 0.8710591197013855\n",
      "Working on batch  230/290\n",
      "Loss: 1.115174412727356\n",
      "Working on batch  231/290\n",
      "Loss: 1.1726415157318115\n",
      "Working on batch  232/290\n",
      "Loss: 1.1762199401855469\n",
      "Working on batch  233/290\n",
      "Loss: 1.0201287269592285\n",
      "Working on batch  234/290\n",
      "Loss: 1.1576788425445557\n",
      "Working on batch  235/290\n",
      "Loss: 1.0197559595108032\n",
      "Working on batch  236/290\n",
      "Loss: 1.168770432472229\n",
      "Working on batch  237/290\n",
      "Loss: 1.0372854471206665\n",
      "Working on batch  238/290\n",
      "Loss: 0.9382604360580444\n",
      "Working on batch  239/290\n",
      "Loss: 0.8476771712303162\n",
      "Working on batch  240/290\n",
      "Loss: 0.9489161968231201\n",
      "Working on batch  241/290\n",
      "Loss: 1.0053943395614624\n",
      "Working on batch  242/290\n",
      "Loss: 0.9679514169692993\n",
      "Working on batch  243/290\n",
      "Loss: 1.1585808992385864\n",
      "Working on batch  244/290\n",
      "Loss: 0.936369001865387\n",
      "Working on batch  245/290\n",
      "Loss: 1.069610595703125\n",
      "Working on batch  246/290\n",
      "Loss: 1.0300929546356201\n",
      "Working on batch  247/290\n",
      "Loss: 1.3540693521499634\n",
      "Working on batch  248/290\n",
      "Loss: 1.167157530784607\n",
      "Working on batch  249/290\n",
      "Loss: 1.0445314645767212\n",
      "Working on batch  250/290\n",
      "Loss: 1.7859021425247192\n",
      "Working on batch  251/290\n",
      "Loss: 1.1637256145477295\n",
      "Working on batch  252/290\n",
      "Loss: 1.008430004119873\n",
      "Working on batch  253/290\n",
      "Loss: 1.237813949584961\n",
      "Working on batch  254/290\n",
      "Loss: 0.7978734970092773\n",
      "Working on batch  255/290\n",
      "Loss: 0.9881115555763245\n",
      "Working on batch  256/290\n",
      "Loss: 1.0477880239486694\n",
      "Working on batch  257/290\n",
      "Loss: 0.9827778339385986\n",
      "Working on batch  258/290\n",
      "Loss: 0.746291995048523\n",
      "Working on batch  259/290\n",
      "Loss: 0.8822585344314575\n",
      "Working on batch  260/290\n",
      "Loss: 1.1748961210250854\n",
      "Working on batch  261/290\n",
      "Loss: 1.4714341163635254\n",
      "Working on batch  262/290\n",
      "Loss: 0.8370769023895264\n",
      "Working on batch  263/290\n",
      "Loss: 0.9454901814460754\n",
      "Working on batch  264/290\n",
      "Loss: 0.988224446773529\n",
      "Working on batch  265/290\n",
      "Loss: 1.0626027584075928\n",
      "Working on batch  266/290\n",
      "Loss: 1.2885349988937378\n",
      "Working on batch  267/290\n",
      "Loss: 1.1716835498809814\n",
      "Working on batch  268/290\n",
      "Loss: 1.1517109870910645\n",
      "Working on batch  269/290\n",
      "Loss: 1.2253013849258423\n",
      "Working on batch  270/290\n",
      "Loss: 1.1992478370666504\n",
      "Working on batch  271/290\n",
      "Loss: 1.1037235260009766\n",
      "Working on batch  272/290\n",
      "Loss: 1.179550290107727\n",
      "Working on batch  273/290\n",
      "Loss: 1.2843899726867676\n",
      "Working on batch  274/290\n",
      "Loss: 1.0209007263183594\n",
      "Working on batch  275/290\n",
      "Loss: 1.2024736404418945\n",
      "Working on batch  276/290\n",
      "Loss: 1.214388370513916\n",
      "Working on batch  277/290\n",
      "Loss: 0.9489830136299133\n",
      "Working on batch  278/290\n",
      "Loss: 1.2205404043197632\n",
      "Working on batch  279/290\n",
      "Loss: 1.1108620166778564\n",
      "Working on batch  280/290\n",
      "Loss: 1.072144627571106\n",
      "Working on batch  281/290\n",
      "Loss: 1.3342124223709106\n",
      "Working on batch  282/290\n",
      "Loss: 1.0135782957077026\n",
      "Working on batch  283/290\n",
      "Loss: 1.1312552690505981\n",
      "Working on batch  284/290\n",
      "Loss: 0.9754310250282288\n",
      "Working on batch  285/290\n",
      "Loss: 0.8611403107643127\n",
      "Working on batch  286/290\n",
      "Loss: 1.2948105335235596\n",
      "Working on batch  287/290\n",
      "Loss: 1.0131161212921143\n",
      "Working on batch  288/290\n",
      "Loss: 0.9721604585647583\n",
      "Working on batch  289/290\n",
      "Loss: 0.9415957927703857\n",
      "Working on batch  290/290\n",
      "Loss: 0.9946507215499878\n",
      "Epoch [1/3] Accuracy: 56.144890038809834%\n",
      "Epoch n: 2/3\n",
      "----------\n",
      "Working on batch  1/290\n",
      "Loss: 1.0745384693145752\n",
      "Working on batch  2/290\n",
      "Loss: 1.0731911659240723\n",
      "Working on batch  3/290\n",
      "Loss: 0.9996415376663208\n",
      "Working on batch  4/290\n",
      "Loss: 1.4677067995071411\n",
      "Working on batch  5/290\n",
      "Loss: 1.293042778968811\n",
      "Working on batch  6/290\n",
      "Loss: 0.9387885332107544\n",
      "Working on batch  7/290\n",
      "Loss: 1.1587367057800293\n",
      "Working on batch  8/290\n",
      "Loss: 1.1802397966384888\n",
      "Working on batch  9/290\n",
      "Loss: 1.4233055114746094\n",
      "Working on batch  10/290\n",
      "Loss: 0.795944094657898\n",
      "Working on batch  11/290\n",
      "Loss: 0.954071581363678\n",
      "Working on batch  12/290\n",
      "Loss: 1.044661283493042\n",
      "Working on batch  13/290\n",
      "Loss: 0.8897296786308289\n",
      "Working on batch  14/290\n",
      "Loss: 0.7111738920211792\n",
      "Working on batch  15/290\n",
      "Loss: 1.0219957828521729\n",
      "Working on batch  16/290\n",
      "Loss: 0.7190371155738831\n",
      "Working on batch  17/290\n",
      "Loss: 1.1295303106307983\n",
      "Working on batch  18/290\n",
      "Loss: 1.3737034797668457\n",
      "Working on batch  19/290\n",
      "Loss: 0.7414621114730835\n",
      "Working on batch  20/290\n",
      "Loss: 1.0484580993652344\n",
      "Working on batch  21/290\n",
      "Loss: 1.3917936086654663\n",
      "Working on batch  22/290\n",
      "Loss: 1.3205889463424683\n",
      "Working on batch  23/290\n",
      "Loss: 1.3047882318496704\n",
      "Working on batch  24/290\n",
      "Loss: 1.0206711292266846\n",
      "Working on batch  25/290\n",
      "Loss: 0.9892833232879639\n",
      "Working on batch  26/290\n",
      "Loss: 1.0507729053497314\n",
      "Working on batch  27/290\n",
      "Loss: 1.0723415613174438\n",
      "Working on batch  28/290\n",
      "Loss: 0.9769212603569031\n",
      "Working on batch  29/290\n",
      "Loss: 1.409079670906067\n",
      "Working on batch  30/290\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch n: {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        print(f'Working on batch  {batch_idx+1}/{len(train_dataloader)}')\n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Loss: {loss.item()}')\n",
    "    torch.save(model.state_dict(), f'{model_dir}/model_epoch_{epoch+1}.pth')\n",
    "    accuracy = evaluate_accuracy(test_dataloader, model)\n",
    "    print(f'Epoch [{epoch+1}/3]'\n",
    "          f' Accuracy: {accuracy}%') \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-29T06:27:08.349780200Z"
    }
   },
   "id": "a74a24d364ca7863"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_roc_curve(dataloader):\n",
    "    model.eval()\n",
    "    test_probs = []\n",
    "    test_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            test_probs.extend(probabilities[:, 1].cpu().numpy())\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_targets, test_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4ae00bfd543f3e3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "timm.create_model()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "39de72f095e3b0e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
